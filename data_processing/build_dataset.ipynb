{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMWdSCyjC8Jqa4sYo9NFQ8d"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"P9BcgScZ_hYf"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["**This file is to build the final augmented dataset used for training model, including two major parts:**\n","\n","\n","**1. Post processing: singal post-processing for both raw data collected from human and robot.** \n","   1. For human: downsampling (decimate) both EMG/IMU to the rate at 10 Hz\n","   2. For robot: downsampling (decimate) one segment of datapoints to 8 points, add 2 static points at the end of each segment. Integrate 4 segments into single sequence. \n","   3. Concatenate 10 repeats for both human and robot data, and then align them to obtain the original dataset.\n","\n","**2. Data augmentation: apply augmentaion on original dataset in order to train the mVAE model**\n","   1. Using sklearn.preprocessing.MinMaxScaler to normailize the original data to the range at [-1, 1]\n","   2. Horizontally concatenate all data points at current time (at t) with them at previous time (at t -1)\n","   3. Split dataset into training set and testing set at ratio of 80:20\n","   4. Mask all robot data in training set with value -2 to obtain the case 2 dataset; mask all original data at t in training set with value -2  to obtain case 3 dataset\n","   5. Vertically concatenate original data with case 2 and case 3 data to obtain the final augmented training set"],"metadata":{"id":"UKRnvFCt3sWI"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import pandas as pd \n","import numpy as np\n","from scipy.signal import detrend\n","from scipy import signal\n","import math\n","from sklearn import preprocessing"],"metadata":{"id":"OADh0S87_wkd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["########### Below is the part for processing emg data ###########\n","### function for getting emg data in one task for one arm \n","def process_emg(i, arm):  \n","  df_emg = pd.read_csv('/content/drive/MyDrive/finalProject/human_/task_'  + str(i) + '-' + arm +'_myo-emg.csv')\n","  #slice rows : 10 trials (40s) in total\n","  df_emg = df_emg.iloc[200:8200,:]\n","\n","  # convert '.data' col to array\n","  df_emg['.data'] = df_emg['.data'].apply(lambda x: np.fromstring(x[1:-1], sep=','))\n","  def f(emgs): \n","      return [emg for i, emg in enumerate(emgs)]\n","\n","  emg_names = ['emg_0', 'emg_1', 'emg_2', 'emg_3', 'emg_4', 'emg_5', 'emg_6', 'emg_7']\n","  df_emg[emg_names] = df_emg.apply(lambda x: f(x['.data']), axis=1, result_type='expand')\n","\n","  # time_elapsed\n","  df_emg['time']=pd.to_datetime(df_emg['time'])\n","  df_emg['time_elapsed']=df_emg['time'].apply(lambda t : t - df_emg.iloc[0,0]).dt.total_seconds()\n","\n","  #post_process \n","  df_emg_proc = pd.DataFrame()\n","  for emg in emg_names:\n","    df_emg[emg] = detrend(df_emg[emg], type='constant')       ##detrend: remove any constant offset\n","    df_emg[emg] = abs(df_emg[emg])                            ##Full-wave rectification: simply just take the absolute values \n","    df_emg[emg] = df_emg[emg].rolling(window=50, min_periods=1).mean()       ##Smoothening: moving average with 250ms sliding window\n","    #downsample to 10hz  (400points for 40s)\n","    q = 20 # downsampling factor\n","    df_emg_proc[emg] = signal.decimate(df_emg[emg], q)\n","  duration = df_emg['time_elapsed'].tail(1).item()\n","  df_emg_proc['time_elapsed']= np.linspace(0, duration, len(df_emg_proc))\n","\n","  return df_emg_proc\n","#################################################################"],"metadata":{"id":"siFYBSfs_xRL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["########### Below is the part for processing imu data ###########\n","### function for getting imu data in one task for one arm \n","def process_imu(i, arm):\n","  df_imu = pd.read_csv('/content/drive/MyDrive/finalProject/human_/task_'  + str(i) + '-' + arm +'_myo-imu.csv')\n","  df_imu = df_imu.iloc[50:2050,:]\n","\n","  # time_elapsed\n","  df_imu['time']=pd.to_datetime(df_imu['time'])\n","  df_imu['time_elapsed']=df_imu['time'].apply(lambda t : t - df_imu.iloc[0,0]).dt.total_seconds()\n","\n","  # post_process\n","  df_imu_proc = pd.DataFrame()\n","  #downsample to 10hz  (400points for 40s)\n","  q = 5 # downsampling factor\n","  #for ori\n","  ori_name = ['.orientation.x', '.orientation.y', '.orientation.z', '.orientation.w']\n","  for ori in ori_name:\n","    df_imu_proc[ori] = signal.decimate(df_imu[ori], q)\n","  #for ang_vel\n","  ang_vel_name = ['.angular_velocity.x', '.angular_velocity.y', '.angular_velocity.z']\n","  for ang_vel in ang_vel_name:\n","    df_imu_proc[ang_vel] = signal.decimate(df_imu[ang_vel], q)\n","  #for lin_acc\n","  lin_acc_name = ['.linear_acceleration.x', '.linear_acceleration.y', '.linear_acceleration.z']\n","  for lin_acc in lin_acc_name:\n","    df_imu_proc[lin_acc] = signal.decimate(df_imu[lin_acc], q)\n","\n","  duration = df_imu['time_elapsed'].tail(1).item()\n","  df_imu_proc['time_elapsed']= np.linspace(0, duration, len(df_imu_proc))\n","\n","  return df_imu_proc\n","###################################################################"],"metadata":{"id":"n-0zvOf2ErvR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["########### Below is the part for processing robot data ###########\n","### function for getting robo data segment: 4 seg in one sequence\n","def build_robo_segment(task, trial, seg):\n","  df_robo = pd.read_csv('/content/drive/MyDrive/finalProject/robot_/' + str(task) + '/trial_' + str(trial) + '_' + str(seg) + '-points_data.csv')\n","  # convert '.data' col to array\n","  df_robo['.position'] = df_robo['.position'].apply(lambda x: np.fromstring(x[1:-1], sep=','))\n","  def split(positions): \n","      return [pos for i, pos in enumerate(positions)]\n","\n","  pos_names = ['pos_1', 'pos_2', 'pos_3', 'pos_4', 'pos_5', 'pos_6', 'pos_7', 'f_pos_1', 'f_pos_2']\n","  df_robo[pos_names] = df_robo.apply(lambda x: split(x['.position']), axis=1, result_type='expand')\n","  #extract joint position data\n","  joints_pos_names = pos_names[:7]\n","  df_robo_pos = df_robo[joints_pos_names]\n","  df_robo_pos['real_time'] = [i / 10 for i in range(len(df_robo_pos))]\n","  #process robo data (decimate and add noise)\n","  df_robo_proc = pd.DataFrame()\n","  for pos in joints_pos_names:\n","    q = math.ceil(len(df_robo_pos)/8) # downsampling factor -> resample to size of 8\n","    df_robo_proc[pos] = signal.decimate(df_robo_pos[pos], q, n=3) # n is the order of filter\n","    # df_robo_proc[pos] += np.random.normal(0,0.02,len(df_robo_proc)) # add G noise\n","  duration = df_robo_pos['real_time'].tail(1).item()\n","  df_robo_proc['proc_time'] = np.linspace(0, duration, len(df_robo_proc))\n","  ## add more static points in th end\n","  end_pos = df_robo_proc.tail(1)\n","  length = len(df_robo_proc)\n","  while length < 10:\n","    df_robo_proc = df_robo_proc.append(end_pos, ignore_index = True)\n","    length += 1\n","  \n","  return df_robo_proc  "],"metadata":{"id":"RQqg5TkRnyVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### function for building integrated robo data: pos and vel for whole single trial\n","def build_robo_integrated(task, trial):\n","  df_robo_seg_list = []\n","  for j in range(4):\n","    df_robo_seg_list.append(build_robo_segment(task, trial, seg=j + 1))\n","  ##integrate one trial\n","  df_robo_integrated = pd.concat(df_robo_seg_list, ignore_index = True)\n","\n","  ### fabricate time stamp and generate vel\n","  df_robo_integrated['proc_time'] = np.linspace(0, 4, len(df_robo_integrated))\n","  joints_pos_names = ['pos_1', 'pos_2', 'pos_3', 'pos_4', 'pos_5', 'pos_6', 'pos_7']\n","  joints_vel_names = ['vel_1', 'vel_2', 'vel_3', 'vel_4', 'vel_5', 'vel_6', 'vel_7']\n","  for pos, vel in zip(joints_pos_names, joints_vel_names):\n","    vel_list = [0] # start from 0 vel\n","    for i in range(len(df_robo_integrated) - 1):\n","      vel_instant = (df_robo_integrated[pos][i+1] - df_robo_integrated[pos][i]) / (df_robo_integrated['proc_time'][i+1] - df_robo_integrated['proc_time'][i])\n","      vel_list.append(vel_instant)\n","    df_robo_integrated[vel] = vel_list\n","\n","  return df_robo_integrated"],"metadata":{"id":"Q8fAor289FU6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### function for getting repeats for 10 trials in one task\n","def process_robo(k):\n","  df_robo_repeat_list = []\n","  for i in range(10):\n","    df_robo_repeat_list.append(build_robo_integrated(task=k, trial=i+1))\n","    \n","  return pd.concat(df_robo_repeat_list, ignore_index=True)\n","########################################################################"],"metadata":{"id":"TaWww5UKEwY6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["########### Below is the part for aligning human and robot data ########\n","### function for getting the original dataset\n","def get_original_dataset(n):  # n is the nums of task\n","  task_list = []\n","  for i in range(n):\n","    df_RL_emg = process_emg(i + 1, 'RL').drop(columns=['time_elapsed'])\n","    df_RU_emg = process_emg(i + 1, 'RU').drop(columns=['time_elapsed'])\n","    df_RL_imu = process_imu(i + 1, 'RL').drop(columns=['time_elapsed'])\n","    df_RU_imu = process_imu(i + 1, 'RU').drop(columns=['time_elapsed'])\n","    df_robo = process_robo(i+1).drop(columns=['proc_time'])\n","    ###align human/robo data for one task\n","    combined_data = [df_RL_imu, df_RL_emg, df_RU_imu, df_RU_emg, df_robo]\n","    df_data_oneTask = pd.concat(combined_data, axis=1)\n","    task_list.append(df_data_oneTask)\n","\n","  df_original_data = pd.concat(task_list, ignore_index=True)\n","  return df_original_data"],"metadata":{"id":"Q8Xa2kHA2Dmu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### obtain the original dataset\n","df_original_dataset = get_original_dataset(5)\n","###save as csv\n","%cd /content/drive/MyDrive/finalProject\n","df_original_dataset.to_csv('original_data_only_t.csv')\n","########################################################################"],"metadata":{"id":"hUBKXqtp49Mt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["########### Below is the part for data augmentation ########\n","### normalize within -1 and 1 -> coln by coln \n","from sklearn.preprocessing import MinMaxScaler\n","\n","scaler = MinMaxScaler(feature_range=(-1, 1)) \n","scaler.fit(df_original_dataset)\n","scaled = scaler.fit_transform(df_original_dataset)\n","scaled_original_dataset = pd.DataFrame(scaled, columns=df_original_dataset.columns)\n","scaled_original_dataset"],"metadata":{"id":"k-73jIYdH851"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["###combine t with t - 1\n","df_original_dataset_prev = scaled_original_dataset.iloc[:-1, :]\n","df_original_dataset_cur = scaled_original_dataset.iloc[1:, :]\n","dataset_list = [df_original_dataset_cur, df_original_dataset_prev]"],"metadata":{"id":"M0uoPkpR8k8m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#[df_RL_imu, df_RL_emg, df_RU_imu, df_RU_emg, df_robo_pos, robo_vel]\n","### function for build a dataset with data at t and at t-1\n","def create_cur_prev_dataset(i):\n","  # 0: cur, 1: prev\n","  RL_imu = dataset_list[i].iloc[:,:10].reset_index(drop=True)\n","  RL_emg = dataset_list[i].iloc[:,10:18].reset_index(drop=True)\n","  RU_imu = dataset_list[i].iloc[:,18:28].reset_index(drop=True)\n","  RU_emg = dataset_list[i].iloc[:,28:36].reset_index(drop=True)\n","  robo_pos = dataset_list[i].iloc[:,36:43].reset_index(drop=True)\n","  robo_vel = dataset_list[i].iloc[:,43:50].reset_index(drop=True)\n","\n","  return [RL_imu, RL_emg, RU_imu, RU_emg, robo_pos, robo_vel]"],"metadata":{"id":"OAB42PGG7DOs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cur_data_list = create_cur_prev_dataset(0)\n","prev_data_list = create_cur_prev_dataset(1)\n","\n","#pd.concat(combined_data, axis=1)\n","item_list = []\n","for i in range(len(cur_data_list)):\n","  item_list.append(cur_data_list[i])\n","  item_list.append(prev_data_list[i])\n","\n","data_with_cur_prev = pd.concat(item_list, axis=1)\n","data_with_cur_prev.to_csv('original_data_with_cur_prev.csv')  # 1999 rows × 100 columns"],"metadata":{"id":"a5UsqCuLB0N-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["####split dataset to train 80%, test 20%\n","from sklearn.model_selection import train_test_split\n","training_data, testing_data = train_test_split(data_with_cur_prev, test_size=0.2)\n","training_data.to_csv('raw_training_data.csv')\n","testing_data.to_csv('testing_data.csv')"],"metadata":{"id":"58QSC-KiLCbE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### load training data\n","# case 1: original data\n","training_data = pd.read_csv('raw_training_data.csv', header=None, skiprows=1, index_col=[0]).reset_index(drop=True) # 1599 rows × 100 columns"],"metadata":{"id":"CU5U6GLwcMdZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### augment for training data\n","# case 2: mask robot data \n","\n","# training_data.reset_index(drop=True, inplace=True)\n","masked_robo = pd.DataFrame(np.full((training_data.shape[0],28),-2))\n","training_no_robo = pd.concat([training_data.iloc[:, :72],masked_robo], axis=1)\n","training_no_robo.columns = training_data.columns\n","training_no_robo.to_csv('training_no_robo.csv') # 1599 rows × 100 columns"],"metadata":{"id":"4uhyVl9FMUbM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# case 3: mask data at t\n","masked_imu_cur = pd.DataFrame(np.full((training_data.shape[0],10),-2))\n","masked_emg_cur = pd.DataFrame(np.full((training_data.shape[0],8),-2))\n","masked_robo_cur = pd.DataFrame(np.full((training_data.shape[0],7),-2))\n","aug_item_list = [masked_imu_cur, training_data.iloc[:,10:20], masked_emg_cur, training_data.iloc[:,28:36],\n","                 masked_imu_cur, training_data.iloc[:,46:56], masked_emg_cur, training_data.iloc[:,64:72],\n","                 masked_robo_cur, training_data.iloc[:,79:86], masked_robo_cur, training_data.iloc[:,93:100]]\n","training_no_cur = pd.concat(aug_item_list, axis=1)\n","training_no_cur.columns = training_data.columns\n","training_no_cur.to_csv('training_no_cur.csv') # 1599 rows × 100 columns"],"metadata":{"id":"bFFB0Qj1Tp00"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##concat case 1, 2, 3\n","aug_training_list = [training_data, training_no_robo, training_no_cur]\n","aug_training_data = pd.concat(aug_training_list, axis=0, ignore_index=True)\n","aug_training_data # 4797 rows × 100 columns"],"metadata":{"id":"JwNE8578WsuB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### aug + original\n","original_training_data_for_label = pd.concat([training_data,training_data,training_data], axis=0, ignore_index=True)\n","final_list = [aug_training_data, original_training_data_for_label]\n","final_aug_training = pd.concat(final_list, axis=1)\n","### save final aug training data\n","final_aug_training.to_csv('final_aug_training_data.csv') # 4797 rows × 200 columns"],"metadata":{"id":"N2ZCXJ0hfQu3"},"execution_count":null,"outputs":[]}]}